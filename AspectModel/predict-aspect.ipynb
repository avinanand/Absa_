{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir ='/kaggle/input/absa-dataset/input/sentihood/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_file = os.path.join(data_dir, 'sentihood-train.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_sentihood_json(in_file):\n    with open(in_file) as f:\n        data = json.load(f)\n    ret = []\n    for d in data:\n        text = d['text']\n        sent_id = d['id']\n        opinions = []\n        targets = set()\n        for opinion in d['opinions']:\n            sentiment = opinion['sentiment']\n            aspect = opinion['aspect']\n            target_entity = opinion['target_entity']\n            targets.add(target_entity)\n            opinions.append((target_entity, aspect, sentiment))\n        ret.append((sent_id, text, opinions))\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\ntrain = parse_sentihood_json(in_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file = os.path.join(data_dir, 'sentihood-test.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = parse_sentihood_json(test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_aspects = ['general', 'price', 'transit-location', 'safety']\ndef convert_input(data):\n    ret = []\n    for sent_id, text, opinions in data:\n        for target_entity, aspect, sentiment in opinions:\n            if aspect not in all_aspects:\n                continue\n            ret.append((sent_id, text, target_entity, aspect, sentiment))\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = convert_input(test)\ntest = pd.DataFrame(test)\n\ntest['id'] = test[0]\ntest['text'] = test[1]\ntest['target_entity'] = test[2]\ntest['label'] = test[3]\ntest['sentiment'] = test[4]\n\ntest = test.drop(0, axis=1)\ntest = test.drop(1, axis = 1)\ntest = test.drop(2, axis = 1)\ntest = test.drop(3, axis=1)\ntest = test.drop(4, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = convert_input(train)\ntrain = pd.DataFrame(train)\n\ntrain['id'] = train[0]\ntrain['text'] = train[1]\ntrain['target_entity'] = train[2]\ntrain['label'] = train[3]\ntrain['sentiment'] = train[4]\n\ntrain = train.drop(0, axis=1)\ntrain = train.drop(1, axis = 1)\ntrain = train.drop(2, axis = 1)\ntrain = train.drop(3, axis=1)\ntrain = train.drop(4, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv(\"to_train_senti.csv\", index=False)\ntest.to_csv(\"to_test_senti.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text'] = test['text'] + ' ' + test['target_entity']\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'] + ' ' + train['target_entity']\n#test_df['sentence1'] = test_df['sentence1'] + ' ' + test_df['sentence2'] + ' ' + test_df['label']\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop([\"target_entity\"],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop([\"target_entity\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby('label').count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('label').count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import transformers\n\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 12\nVALID_BATCH_SIZE = 12\nEPOCHS = 10\nBERT_PATH = \"../input/bert-base-uncased/\"\nMODEL_PATH = \"model.bin\"\nTRAINING_FILE = \"../input/absa-dataset/input/bert-pair/train_NLI_M.tsv\"\nDEV_FILE = \"../input/absa-dataset/input/bert-pair/dev_NLI_M.tsv\"\nTEST_FILE = \"../input/absa-dataset/input/bert-pair/test_NLI_M.tsv\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nclass BERTDataset:\n    def __init__(self, sentence1s,targets):\n        self.sentence1s = sentence1s\n        #self.sentence2s = sentence2s\n        self.targets = targets\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.sentence1s)\n                   \n        \n    def __getitem__(self, item):\n        sentence1 = str(self.sentence1s[item])\n        sentence1 = \" \".join(sentence1.split())\n        \n        #sentence2 = str(self.sentence2s[item])\n        #sentence2 = \" \".join(sentence2.split())\n\n        inputs = self.tokenizer.encode_plus(sentence1,\n                                            None,\n                                            add_special_tokens=True, \n                                            max_length=self.max_len,\n                                            pad_to_max_length=True,\n                                           )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(self.targets[item], dtype=torch.long),\n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nimport torch.nn as nn\n\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 4)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport pdb\n\n\ndef loss_fn(outputs, targets):\n    #targets = torch.argmax(targets, 1)\n    #loss = nn.CrossEntropyLoss()(outputs, targets.view(-1, 1))\n    loss = nn.CrossEntropyLoss()(outputs, targets)\n    \n    return loss\n\n\ndef train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        #pdb.set_trace()\n        targets = targets.to(device, dtype=torch.long)\n        #pdb.set_trace()\n\n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.long)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            act = nn.Softmax(dim=1)\n            outputs = act(outputs)\n            fin_targets.extend(targets.detach().cpu().numpy().tolist())\n            fin_outputs.extend(outputs.cpu().numpy().tolist())\n            \n    return fin_outputs, fin_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\n\n#from model import BERTBaseUncased\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn import preprocessing\n\ndef run():\n    \n    encoder = preprocessing.LabelEncoder()\n    train.loc[:, \"label\"] = encoder.fit_transform(train[\"label\"])\n    #dev_df.loc[:, \"label\"] = encoder.transform(dev_df[\"label\"])\n    test.loc[:, \"label\"] = encoder.transform(test[\"label\"])\n    \n    \n    train_dataset = BERTDataset(sentence1s = train.text.values, targets=train.label.values)\n    \n    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4)\n    \n    valid_dataset = BERTDataset(sentence1s = test.text.values,targets=test.label.values)\n    \n    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=4)\n    \n    device = torch.device(\"cuda\")\n    model = BERTBaseUncased()\n    model.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [{\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.001,},\n                            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},]\n    num_train_steps = int(len(train) / TRAIN_BATCH_SIZE * EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n    \n    model = nn.DataParallel(model)\n    \n    best_accuracy = 0\n    \n    for epoch in range(EPOCHS):\n        print(epoch)\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n        outputs, targets = eval_fn(valid_data_loader, model, device)\n        #pdb.set_trace()\n        outputs = torch.tensor(outputs)\n        outputs = torch.argmax(outputs, dim=1)\n        #outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f\"Accuracy Score =\", {accuracy})\n        if accuracy > best_accuracy:\n            torch.save(model.state_dict(), MODEL_PATH)\n            best_accuracy = accuracy\n            \nif __name__ == \"__main__\":\n    run()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data1 = pd.read_csv(\"../input/absa-dataset/input/bert-pair/dev_NLI_M.tsv\", sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data1['target'] = data1['sentence2'].apply(lambda x: x.split('-')[2])\n# data1['1_2'] = data1['sentence2'].apply(lambda x: x.split('-')[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data1['target'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data1.head(-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def changeForFun(data1):\n#     rest = []\n#     for index, row in data1.iterrows():\n#         response = {}\n#         ids = row['id']\n#         text = row['sentence1']\n#         target_aspect = row['sentence2']\n#         sentiment = row['label']\n\n#         response = {\n#             \"opinions\": [{\n#                 \"sentiment\": sentiment,\n#                 \"aspect\": target_aspect,\n#                 \"target_entity\": target_aspect}\n#             ],\n#             \"id\": ids,\n#             \"text\": text\n#         }\n#         rest.append(response)\n#     return rest\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rest = changeForFun(data1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(rest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}